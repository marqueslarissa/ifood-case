{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7651dd7-415c-4d26-9145-135b676b5b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "def KNNImputer(df, target_cols, features_ref, k=5):\n",
    "    \"\"\"\n",
    "    Imputa valores ausentes nas colunas `target_cols` usando KNN via distancia euclidiana com MinMax scaling.\n",
    "\n",
    "    - df: DataFrame de entrada\n",
    "    - target_cols: Lista de colunas a imputar\n",
    "    - features_ref: Lista de colunas confiaveis (sem missing) para basear a distancia\n",
    "    - k: Numero de vizinhos mais proximos (default = 5)\n",
    "    \"\"\"\n",
    "\n",
    "    # Vetorizar colunas de referencia\n",
    "    assembler = VectorAssembler(inputCols=features_ref, outputCol=\"features_vec\")\n",
    "    df_vec = assembler.transform(df)\n",
    "\n",
    "    # Normalizar os vetores\n",
    "    scaler = MinMaxScaler(inputCol=\"features_vec\", outputCol=\"scaled_features\")\n",
    "    scaler_model = scaler.fit(df_vec)\n",
    "    df_scaled = scaler_model.transform(df_vec)\n",
    "\n",
    "    # Converter vetor em array para facilitar o calculo de distancia\n",
    "    df_scaled = df_scaled.withColumn(\"scaled_array\", vector_to_array(\"scaled_features\"))\n",
    "\n",
    "    for col_target in target_cols:\n",
    "        print(f\"Imputando coluna: {col_target}\")\n",
    "\n",
    "        df_missing = df_scaled.filter(col(col_target).isNull())\n",
    "        df_known = df_scaled.filter(col(col_target).isNotNull())\n",
    "\n",
    "        if df_missing.count() == 0:\n",
    "            print(f\"Coluna {col_target} n√£o tem valores nulos.\")\n",
    "            continue\n",
    "\n",
    "        # Cross join\n",
    "        joined = df_missing.alias(\"a\").crossJoin(df_known.alias(\"b\"))\n",
    "\n",
    "        # Calcular distancia euclidiana\n",
    "        dist_expr = sqrt(\n",
    "            reduce(operator.add,\n",
    "                   [pow(col(\"a.scaled_array\")[i] - col(\"b.scaled_array\")[i], 2) for i in range(len(features_ref))]\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "        joined = joined.withColumn(\"distance\", dist_expr)\n",
    "\n",
    "        # Janela para pegar os k vizinhos mais proximos\n",
    "        window = Window.partitionBy(\"a.id\").orderBy(\"distance\")\n",
    "        knn_topk = joined.withColumn(\"rank\", row_number().over(window)) \\\n",
    "                         .filter(col(\"rank\") <= k)\n",
    "\n",
    "        # Verifica tipo da coluna (string = categorica usa moda, senao media usando knn)\n",
    "        if dict(df.dtypes)[col_target] == \"string\":\n",
    "            # Moda (valor mais frequente)\n",
    "            knn_mode = knn_topk.groupBy(\"a.id\", f\"b.{col_target}\") \\\n",
    "                .agg(count(\"*\").alias(\"freq\")) \\\n",
    "                .withColumn(\"rank\", row_number().over(Window.partitionBy(\"id\").orderBy(col(\"freq\").desc()))) \\\n",
    "                .filter(col(\"rank\") == 1) \\\n",
    "                .select(\"id\", col(f\"b.{col_target}\").alias(f\"{col_target}_imputed\"))\n",
    "\n",
    "            df_scaled = df_scaled.join(knn_mode, on=\"id\", how=\"left\") \\\n",
    "                .withColumn(col_target, coalesce(col(col_target), col(f\"{col_target}_imputed\"))) \\\n",
    "                .drop(f\"{col_target}_imputed\")\n",
    "\n",
    "        else:\n",
    "            # Media KNN\n",
    "            knn_avg = knn_topk.groupBy(\"a.id\") \\\n",
    "                .agg(avg(col(f\"b.{col_target}\")).alias(f\"{col_target}_imputed\"))\n",
    "\n",
    "            df_scaled = df_scaled.join(knn_avg, on=\"id\", how=\"left\") \\\n",
    "                .withColumn(col_target, coalesce(col(col_target), col(f\"{col_target}_imputed\"))) \\\n",
    "                .drop(f\"{col_target}_imputed\")\n",
    "\n",
    "    return df_scaled.drop(\"features_vec\", \"scaled_features\", \"scaled_array\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "knn_imputer_pyspark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}